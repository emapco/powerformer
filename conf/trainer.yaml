trainer: # transformers.TrainingArguments
  num_train_epochs: 100
  eval_strategy: steps
  eval_steps: 50
  logging_steps: 10
  logging_strategy: steps
  output_dir: training_output
  overwrite_output_dir: false
  save_strategy: steps
  save_steps: 50
  save_total_limit: 10
  warmup_ratio: 0.0
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  report_to: wandb
  run_name: powerformer
  dataloader_num_workers: 8
  dataloader_prefetch_factor: 2
  dataloader_pin_memory: true
  lr_scheduler_type: constant_with_warmup
  learning_rate: 1.0e-4
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  optim: adamw_torch
  tf32: false
  fp16: true
  fp16_full_eval: false # transformer<=4.49.0 contains an bug when hyperparameter tuning with fp16_full_eval=true and optuna backend
  torch_compile: false # set to false when hyperparameter tuning
  # custom
  # Scaled weight decay for adamw optimizer - https://arxiv.org/pdf/1711.05101.pdf
  use_scaled_weight_decay: true

hyperopt:
  hp_space:
    training:
      - name: warmup_ratio
        type: float
        low: 0.0
        high: 0.1
        step: 0.02
      - name: per_device_train_batch_size
        type: int
        low: 8
        high: 128
        step: 8
      - name: learning_rate
        type: float
        low: 5e-5
        high: 1e-3
        step: 1e-5
      - name: gradient_accumulation_steps
        type: int
        low: 2
        high: 16
        step: 2
